
# 2024-02-28: 5X Faster QLoRA Fine-tuning with Unsloth

[![Back to TIL Hub](https://img.shields.io/badge/←%20Back%20to-TIL%20Hub-blue?style=for-the-badge)](README.md)

> Unsloth provides 5X faster QLoRA fine-tuning with 60% less memory usage through optimized kernels and efficient implementations, dramatically reducing training time and resource requirements for LLM fine-tuning.

## The Pain Point

Traditional LoRA/QLoRA fine-tuning faces several challenges:

- Slow training speed with standard implementations
- High memory consumption even with quantization
- Complex setup and configuration requirements
- Inefficient kernel operations in standard libraries
- Long iteration cycles during experimentation

Unsloth solves these problems with optimized implementations that maintain compatibility while dramatically improving performance.

## Step-by-Step Guide

### 1. Install Unsloth Environment

```bash
# Create dedicated conda environment
conda create --name unsloth_env python=3.10
conda activate unsloth_env

# Install PyTorch with CUDA support
conda install pytorch cudatoolkit torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia

# Install required dependencies
conda install xformers -c xformers
pip install bitsandbytes

# Install Unsloth
pip install "unsloth[conda] @ git+https://github.com/unslothai/unsloth.git"
```

### 2. Load Pre-quantized Model

```python
from unsloth import FastLanguageModel
import torch

# Available 4-bit pre-quantized models (4x faster downloading)
fourbit_models = [
    "unsloth/mistral-7b-bnb-4bit",
    "unsloth/llama-2-7b-bnb-4bit", 
    "unsloth/llama-2-13b-bnb-4bit",
    "unsloth/codellama-34b-bnb-4bit",
    "unsloth/tinyllama-bnb-4bit",
]

max_seq_length = 2048  # Supports RoPE scaling internally

# Load model with Unsloth optimizations
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/mistral-7b-bnb-4bit",
    max_seq_length=max_seq_length,
    dtype=None,
    load_in_4bit=True,
)
```

### 3. Configure Fast LoRA

```python
# Apply Unsloth's optimized LoRA implementation
model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # LoRA rank
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_alpha=16,
    lora_dropout=0,  # Set to 0 for optimization
    bias="none",     # "none" is optimized
    use_gradient_checkpointing=True,
    random_state=3407,
    max_seq_length=max_seq_length,
    use_rslora=False,    # Rank stabilized LoRA support
    loftq_config=None,   # LoftQ support
)
```

### 4. Prepare Training Data

```python
from datasets import load_dataset

# Example with LAION OIG dataset
url = "https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl"
dataset = load_dataset("json", data_files={"train": url}, split="train")

# For custom data, ensure it has a "text" field
# dataset = load_dataset("your_dataset")
```

### 5. Configure and Train

```python
from trl import SFTTrainer
from transformers import TrainingArguments

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=10,
        max_steps=60,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        output_dir="outputs",
        optim="adamw_8bit",
        seed=3407,
    ),
)

# Start training with Unsloth optimizations
trainer.train()
```

### 6. Save and Export Models

```python
# Save LoRA adapter
model.save_pretrained("lora_model")

# Save to GGUF format for inference
model.save_pretrained_gguf("model", tokenizer, quantization_method="q4_k_m")

# Merge and save as 16-bit for vLLM
model.save_pretrained_merged("model", tokenizer, save_method="merged_16bit")
```

## Troubleshooting

### Installation Issues

- Ensure CUDA version matches PyTorch installation
- Use conda for dependency management to avoid conflicts
- Verify GPU compute capability is supported (≥6.1)
- Check that xformers is properly compiled for your CUDA version

### Memory Problems

- Reduce batch size and increase gradient accumulation steps
- Use gradient checkpointing: `use_gradient_checkpointing=True`
- Set LoRA dropout to 0 for memory optimization
- Consider smaller models or lower sequence lengths

### Training Performance Issues

- Ensure you're using pre-quantized models from Unsloth
- Set `lora_dropout=0` and `bias="none"` for maximum speed
- Use bf16 on supported hardware, fp16 otherwise
- Monitor GPU utilization to identify bottlenecks

## Related Resources

- [Unsloth GitHub Repository](https://github.com/unslothai/unsloth) - Main project and documentation
- [Unsloth Model Hub](https://huggingface.co/unsloth) - Pre-quantized models collection
- [Kaggle Notebook Example](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook) - Complete training example
- [Unsloth Wiki](https://github.com/unslothai/unsloth/wiki) - Advanced tips and techniques
- [Fine-tuning Guide with LoRA](https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora) - Comprehensive background on LoRA methods
