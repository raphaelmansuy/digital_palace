# TIL: 5X Faster QLoRA Fine-tuning with Unsloth (2024-02-28)

[![Back to TIL Hub](https://img.shields.io/badge/←%20Back%20to-TIL%20Hub-blue?style=for-the-badge)](README.md)

> **Unsloth provides 5X faster QLoRA fine-tuning with 60% less memory usage** – Optimized kernels and efficient implementations dramatically reduce training time and resource requirements for LLM fine-tuning.

---

## The Pain Point

Traditional LoRA/QLoRA fine-tuning faces several challenges:

- Slow training speed with standard implementations
- High memory consumption even with quantization
- Complex setup and configuration requirements
- Inefficient kernel operations in standard libraries
- Long iteration cycles during experimentation

Unsloth solves these problems with optimized implementations that maintain compatibility while dramatically improving performance.

---

## Step-by-Step Guide

### 1. Install Unsloth Environment

```bash
# Create dedicated conda environment
conda create --name unsloth_env python=3.10
conda activate unsloth_env

# Install PyTorch with CUDA support
conda install pytorch cudatoolkit torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia

# Install required dependencies
conda install xformers -c xformers
pip install bitsandbytes

# Install Unsloth
pip install "unsloth[conda] @ git+https://github.com/unslothai/unsloth.git"
```

### 2. Load Pre-quantized Model

```python
from unsloth import FastLanguageModel
import torch

# Available 4-bit pre-quantized models (4x faster downloading)
fourbit_models = [
    "unsloth/mistral-7b-bnb-4bit",
    "unsloth/llama-2-7b-bnb-4bit", 
    "unsloth/llama-2-13b-bnb-4bit",
    "unsloth/codellama-34b-bnb-4bit",
    "unsloth/tinyllama-bnb-4bit",
]

max_seq_length = 2048  # Supports RoPE scaling internally

# Load model with Unsloth optimizations
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/mistral-7b-bnb-4bit",
    max_seq_length=max_seq_length,
    dtype=None,
    load_in_4bit=True,
)
```

### 3. Configure Fast LoRA

```python
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing=True,
    random_state=3407,
    max_seq_length=max_seq_length,
    use_rslora=False,
    loftq_config=None,
)
```

### 4. Prepare Training Data

```python
from datasets import load_dataset
url = "https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl"
dataset = load_dataset("json", data_files={"train": url}, split="train")
# For custom data, ensure it has a "text" field
```

### 5. Configure and Train

```python
from trl import SFTTrainer
from transformers import TrainingArguments
trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=TrainingArguments(
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        warmup_steps=10,
        max_steps=60,
        fp16=not torch.cuda.is_bf16_supported(),
        bf16=torch.cuda.is_bf16_supported(),
        logging_steps=1,
        output_dir="outputs",
        optim="adamw_8bit",
        seed=3407,
    ),
)
trainer.train()
```

### 6. Save and Export Models

```python
model.save_pretrained("lora_model")
model.save_pretrained_gguf("model", tokenizer, quantization_method="q4_k_m")
model.save_pretrained_merged("model", tokenizer, save_method="merged_16bit")
```

---

## Troubleshooting

### Installation Issues

- Ensure CUDA version matches PyTorch installation
- Use conda for dependency management to avoid conflicts
- Verify GPU compute capability is supported (≥6.1)
- Check that xformers is properly compiled for your CUDA version

### Memory Problems

- Reduce batch size and increase gradient accumulation steps
- Use gradient checkpointing: `use_gradient_checkpointing=True`
- Set LoRA dropout to 0 for memory optimization
- Consider smaller models or lower sequence lengths

### Training Performance Issues

- Ensure you're using pre-quantized models from Unsloth
- Set `lora_dropout=0` and `bias="none"` for maximum speed
- Use bf16 on supported hardware, fp16 otherwise
- Monitor GPU utilization to identify bottlenecks

---

## Security Considerations

- Never share training data or model weights containing sensitive information.
- Use isolated environments for training to avoid dependency conflicts.
- Keep Unsloth and CUDA libraries up to date to avoid vulnerabilities.

---

## Related Resources

- [Unsloth GitHub Repository](https://github.com/unslothai/unsloth)
- [Unsloth Model Hub](https://huggingface.co/unsloth)
- [Kaggle Notebook Example](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)
- [Unsloth Wiki](https://github.com/unslothai/unsloth/wiki)
- [Fine-tuning Guide with LoRA](https://www.mercity.ai/blog-post/guide-to-fine-tuning-llms-with-lora-and-qlora)

---

*⚡ Pro tip: Use Unsloth's pre-quantized models and optimized LoRA settings for the fastest, most memory-efficient LLM fine-tuning!*
