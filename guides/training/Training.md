

# Build a Large Language Model (From Scratch)

From Sebastian Raschka

[](https://github.com/rasbt/LLMs-from-scratch/tree/main#build-a-large-language-model-from-scratch)

This repository contains the code for developing, pretraining, and finetuning a GPT-like LLM and is the official code repository for the book [Build a Large Language Model (From Scratch)](http://mng.bz/orYv).

  
  

[![](https://camo.githubusercontent.com/e2a84773e158a3dae60743fd95c3da1c73b74ee60dccd32b236030ccec55d705/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636f7665722e6a70673f313233)](http://mng.bz/orYv)

  

In [_Build a Large Language Model (From Scratch)_](http://mng.bz/orYv), you'll learn and understand how large language models (LLMs) work from the inside out by coding them from the ground up, step by step. In this book, I'll guide you through creating your own LLM, explaining each stage with clear text, diagrams, and examples.

The method described in this book for training and developing your own small-but-functional model for educational purposes mirrors the approach used in creating large-scale foundational models such as those behind ChatGPT. In addition, this book includes code for loading the weights of larger pretrained models for finetuning.

- Link to the official [source code repository](https://github.com/rasbt/LLMs-from-scratch)
- [Link to the book at Manning](http://mng.bz/orYv)
- [Link to the book page on Amazon](https://www.amazon.com/gp/product/1633437167)
- ISBN 9781633437166
https://github.com/rasbt/LLMs-from-scratch/tree/main