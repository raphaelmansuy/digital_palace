
# Pre-train a model

## Resources

- [Colossal AI return of experience to continue the pre-training of LLama2](https://github.com/hpcaitech/ColossalAI/tree/main/applications/Colossal-LLaMA-2)
- [Colossal AI](https://github.com/hpcaitech/ColossalAI/tree/main) Making large AI models cheaper, faster and more accessible
- [Colossal AI Website](https://colossalai.org/)


```cardlink
url: https://colossalai.org/
title: "Colossal-AI"
description: "Making big AI models cheaper, easier, and scalable"
host: colossalai.org
favicon: https://colossalai.org/img/favicon.ico
image: https://colossalai.org/img/social-card.png
```

```cardlink
url: https://github.com/hpcaitech/ColossalAI/tree/main/applications/Colossal-LLaMA-2
title: "ColossalAI/applications/Colossal-LLaMA-2 at main Â· hpcaitech/ColossalAI"
description: "Making large AI models cheaper, faster and more accessible - hpcaitech/ColossalAI"
host: github.com
favicon: https://github.githubassets.com/favicons/favicon.svg
image: https://repository-images.githubusercontent.com/422274596/bf53ba49-de0d-4308-bc38-9c0ef56f4eeb
```


- [Simple and Scalable Strategies to Continually Pre-train Large Language Models](https://arxiv.org/pdf/2403.08763.pdf)
- [Yi: Open Foundation Models by 01.AI](https://arxiv.org/pdf/2403.04652.pdf)
