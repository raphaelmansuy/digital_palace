# AI: Unlocking the Power of Private Data with Open-Source Bonito ... generate your training dataset from private data ...  
  
Specialized domains like healthcare, law, and finance stand to gain tremendously from large language models.  
  
However, a lack of annotated training data makes adapting these models difficult.  
  
Enter Bonito - an open-source solution for generating synthetic training datasets from private, unannotated text.  
  
In a new paperÂ [arxiv.org/abs/2402.18334](http://arxiv.org/abs/2402.18334), researchers from Brown University introduce "Bonito, a model for conditional task generation". By remixing existing datasets into *meta-templates*, Bonito can convert unannotated text into customized training data for instruction tuning.  
  
ðŸ‘‰ Performance Leaps Through Synthetic Data  
  
The results are striking:  
  
- "22.1 F1 point average increase" when adapting Mistral, Llama, and other models on tasks like QA and NLI  
- "Outperforms self-supervision" by 33.1 F1 points on average  
- "Surpasses larger models" like 11B parameter Flan-T5-XXL  
  
ðŸ‘‰ Practical AI for Real-World NeedsÂ   
  
Bonito also translates into concrete benefits:  
  
- "Domain adaptation" without annotations  
- "Privacy preservation" for sensitive data  
- "Cost savings" compared to API-based models  
  
This research highlights the potential of synthetic data. As models continue to advance, high-quality, customized datasets will prove key to unlocking specialized domains.  
  
Â ðŸ‘‰ Join the Open Community  
  
One of Bonito's most exciting aspects is its public availability. The authors have contributed the full codebase, model, and dataset to the community.  
  
What could you build with an open conditional task generator? Consider getting involved to drive the next wave of innovation!  
  
Let me know what you think of Bonito and how we can apply this technology. What other private-data domains could benefit from synthetic dataset generation?

[Post](https://www.linkedin.com/feed/update/urn:li:activity:7169517560147750912/)
