# Squeezing Every Last Bit Out of Model Fine-Tuning

Fine-tuning has become the go-to technique for customizing large language models (LLMs) for specific tasks. However, each fine-tuned model comes at a high cost in terms of storage and serving - until now. 

Introducing **BitDelta**, a new method that compresses fine-tuning adjustments down to just **1 bit** with minimal accuracy loss! ðŸ¤¯

## Key Benefits

âœ… **10x+ memory savings** - Enable efficient multi-tenant serving from a single base model

âœ… **2x speedup** in latency via specialized kernels  

âœ… **State-of-the-art performance** retained even for 70B parameter models

## How It Works

BitDelta decomposes fine-tuned weights into the base model weights + a delta. This delta is then quantized to 1-bit precision, with only a scalar factor kept in high precision. 

Despite this drastic compression, performance remains on par with the original fine-tuned models, thanks to an additional distillation step that recalibrates the scales. 

## Real-World Impact

BitDelta paves the way for **democratized access to customized LLMs**. By allowing inexpensive storage and serving of multiple fine-tuned models, we unlock new possibilities:

- **Cloud AI services** can efficiently serve a multitude of customized models
- **Edge devices** can now feasibly utilize specialized on-device models  
- **Startups** can deploy LLMs for niche verticals that were previously cost-prohibitive

The potential is vast. BitDelta brings us one bit closer to an AI-powered future.

*BitDelta is available open-source at: [link](https://github.com/FasterDecoding/BitDelta). Check it out and share your thoughts!*

[Article](https://pli.princeton.edu/blog/2024/bitdelta-your-fine-tune-may-only-be-worth-one-bit)
[Research article](https://arxiv.org/pdf/2402.10193.pdf)
[LinkedIn](https://www.linkedin.com/feed/update/urn:li:activity:7167401001539346432/)

