# Observability

Observability in AI systems refers to the ability to understand, monitor, and debug AI applications through metrics, logs, traces, and analytics. It's essential for maintaining reliability, performance, and trust in production AI systems.

---


## üìñ Learn More

- [LangSmith Observability](https://smith.langchain.com/) ‚Äî LangChain's monitoring platform
- [Weights & Biases](https://wandb.ai/) ‚Äî Experiment tracking and model monitoring
- [MLflow](https://mlflow.org/) ‚Äî ML lifecycle management and monitoring
- [OpenTelemetry for AI](https://opentelemetry.io/) ‚Äî Observability standards for AI systems
- [Arize AI](https://arize.com/) ‚Äî ML observability and monitoring platform
- [Amazon Bedrock Agent Observability Tools](https://github.com/awslabs/amazon-bedrock-agent-samples/tree/main/examples/agent_observability) ‚Äî Open-source tools for tracing and monitoring AWS Bedrock Agents
- [Trace Bedrock Agents with Langfuse](https://langfuse.com/docs/integrations/bedrock/example-bedrock-agents) ‚Äî Guide to tracing and monitoring Bedrock Agents using Langfuse and OpenTelemetry


---


## üõ†Ô∏è Key Frameworks & Tools

- [LangSmith](https://smith.langchain.com/) ‚Äî Debug, test, and monitor LLM applications
- [Weights & Biases](https://wandb.ai/) ‚Äî Experiment tracking and model monitoring
- [MLflow](https://mlflow.org/) ‚Äî Open source ML lifecycle management
- [Helicone](https://helicone.ai/) ‚Äî LLM observability and monitoring
- [LangWatch](https://langwatch.ai/) ‚Äî Real-time LLM application monitoring
- [Langfuse](https://langfuse.com/) ‚Äî Open-source LLM observability, tracing, and analytics platform
- [OpenTelemetry](https://opentelemetry.io/) ‚Äî Open standard for distributed tracing and observability
- [OpenInference](https://github.com/openinference/openinference) ‚Äî Semantic conventions for AI observability


---


## üß† Core Concepts

- **Monitoring:** [Production Deployment](./production-deployment.md), [Monitoring Guide](../guides/monitoring.md)
- **Debugging:** [LangSmith](https://smith.langchain.com/), [Mental Models](./mental-models.md)
- **Analytics:** [Performance tracking](../guides/monitoring.md), [Best Practices](../guides/best-practices.md)
- **Reliability:** [Production systems](./production-deployment.md), [Cloud Platforms](./cloud-platforms.md)
- **Compliance:** [AI Legal & Regulatory](./ai-legal-regulatory.md), [Audit Trails](./ai-legal-regulatory.md)
- **Agent Observability:** [Amazon Bedrock Agent Observability Tools](https://github.com/awslabs/amazon-bedrock-agent-samples/tree/main/examples/agent_observability), [Langfuse Bedrock Integration](https://langfuse.com/docs/integrations/bedrock/example-bedrock-agents)


---

## üîó **Related Concepts**

- **[AI Legal & Regulatory Compliance](./ai-legal-regulatory.md)** ‚Äî Compliance monitoring and audit requirements
- **[MLOps](./mlops.md)** ‚Äî Model lifecycle and governance
- **[Production Deployment](./production-deployment.md)** ‚Äî Production monitoring and reliability
- **[AI Testing](./ai-testing.md)** ‚Äî Quality assurance and validation

---


## üöÄ Best Practices & Next Steps

- Start with [Monitoring Guide](../guides/monitoring.md)
- Implement [LangSmith](https://smith.langchain.com/) for LLM monitoring
- Explore [Weights & Biases](https://wandb.ai/) for experiment tracking
- See [Production Deployment](./production-deployment.md) for integration
- For AWS Bedrock Agents, see [Amazon Bedrock Agent Observability Tools](https://github.com/awslabs/amazon-bedrock-agent-samples/tree/main/examples/agent_observability) and [Langfuse Bedrock Integration Guide](https://langfuse.com/docs/integrations/bedrock/example-bedrock-agents)

[Back to Concepts Hub](./README.md)
