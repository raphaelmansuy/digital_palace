# Contextual Retrieval (Anthropic)

Contextual Retrieval is an advanced technique for improving Retrieval-Augmented Generation (RAG) systems, introduced by Anthropic for use with Claude and other LLMs. It addresses the limitations of traditional RAG by preserving and leveraging document context, resulting in significantly higher retrieval accuracy and better downstream model performance.

---

## Key Concepts

- **Traditional RAG Limitations:** Standard RAG splits documents into small chunks and retrieves them using embeddings and/or BM25. This often loses important context, leading to ambiguous or incomplete retrievals.
- **Contextual Retrieval:** Prepends a chunk-specific, document-aware context to each chunk before embedding and indexing. This context is generated by prompting an LLM (e.g., Claude) to summarize the chunk's role within the whole document.
- **Contextual Embeddings:** Embeddings are created from the contextualized chunk, improving semantic search.
- **Contextual BM25:** The same context is used for lexical search, improving exact match retrieval.
- **Reranking:** Further boosts performance by reranking retrieved chunks for relevance using a reranker model (e.g., Cohere Reranker).

---

## Implementation Overview

1. **Chunking:** Split documents into small, overlapping chunks (e.g., 200-800 tokens).
2. **Contextualization:** For each chunk, use an LLM to generate a short context situating the chunk within the document:

   ```
   <document>
   {{WHOLE_DOCUMENT}}
   </document>
   Here is the chunk we want to situate within the whole document
   <chunk>
   {{CHUNK_CONTENT}}
   </chunk>
   Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.
   ```

3. **Prepend Context:** Prepend the generated context to the chunk text.
4. **Indexing:**
   - Create embeddings for each contextualized chunk (Contextual Embeddings)
   - Index chunks with BM25 using the contextualized text (Contextual BM25)
5. **Retrieval:**
   - For a query, retrieve top chunks using both embeddings and BM25
   - Combine and deduplicate results (rank fusion)
   - Optionally rerank top-N chunks for relevance
6. **Prompt Construction:** Add the top-K chunks to the LLM prompt for answer generation.

---

## Performance

- **Contextual Embeddings** reduce retrieval failure rate by 35%
- **Contextual Embeddings + Contextual BM25** reduce failure by 49%
- **Adding reranking** reduces failure by 67%
- Best results: combine contextual embeddings, contextual BM25, and reranking, and pass top-20 chunks to the model

---

## Practical Tips

- Use prompt caching to reduce LLM costs when generating chunk contexts
- Experiment with chunk size, overlap, and number of chunks retrieved (top-20 is often optimal)
- Tailor the contextualization prompt for your domain for best results
- Evaluate retrieval quality with recall@K and downstream task performance

---

## Resources & References

- [Anthropic Contextual Retrieval Engineering Blog](https://www.anthropic.com/engineering/contextual-retrieval)
- [Anthropic Contextual Embeddings Cookbook](https://github.com/anthropics/anthropic-cookbook/tree/main/skills/contextual-embeddings)
- [Prompt Caching Cookbook](https://github.com/anthropics/anthropic-cookbook/blob/main/misc/prompt_caching.ipynb)
- [Appendix II: Example Q&A and Results](https://assets.anthropic.com/m/1632cded0a125333/original/Contextual-Retrieval-Appendix-2.pdf)
- [BM25 (Wikipedia)](https://en.wikipedia.org/wiki/Okapi_BM25)
- [Cohere Reranker](https://cohere.com/rerank)
- [VoyageAI](https://www.voyageai.com/)
- [Gemini Embeddings](https://ai.google.dev/gemini-api/docs/embeddings)

---

## See Also

- [RAG](./rag.md)
- [Embeddings](./embeddings.md)
- [Knowledge Management](./knowledge-management.md)
- [LLMs](./llms.md)

---

[Back to Concepts Hub](./README.md)
