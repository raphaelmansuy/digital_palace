# Inference

Inference is the process of running a trained AI model (such as an LLM) to generate predictions, outputs, or responses based on new input data. Efficient inference is critical for deploying AI in production, ensuring low latency, scalability, and cost-effectiveness.

---

## üìñ Learn More

- [Model Serving & Inference](../reference/core-technologies.md#model-serving--inference)
- [Specialized Inference Solutions](../reference/core-technologies.md#specialized-inference)
- [Cloud & Distributed Deployment](../reference/core-technologies.md#cloud--distributed-deployment)
- [AI Tools Master Directory](../tools/ai-tools-master-directory.md#model-serving--inference)

---

## üõ†Ô∏è Key Frameworks & Tools

- [vLLM](https://github.com/vllm-project/vllm) ‚Äî High-performance LLM serving
- [Ollama](https://github.com/ollama/ollama) ‚Äî Local LLM runtime
- [MLX Omni Server](https://github.com/madroidmaq/mlx-omni-server) ‚Äî Apple Silicon optimized
- [SkyPilot](https://skypilot.readthedocs.io/en/latest/) ‚Äî Multi-cloud deployment
- [LoraX](https://github.com/predibase/lorax) ‚Äî Multi-LoRA inference server

---

## üß† Core Concepts

- **Serving Architectures:** [Model Serving & Inference](../reference/core-technologies.md#model-serving--inference)
- **Cloud & Edge Deployment:** [Cloud & Distributed Deployment](../reference/core-technologies.md#cloud--distributed-deployment)
- **Optimization:** [Model Optimization](../reference/core-technologies.md#model-optimization)

---

## üöÄ Best Practices & Next Steps

- Start with [Model Serving & Inference](../reference/core-technologies.md#model-serving--inference)
- Explore [AI Tools Master Directory](../tools/ai-tools-master-directory.md#model-serving--inference)
- See [Learning Pathways](./learning-pathways.md) for skill progression

[Back to Concepts Hub](./README.md)
